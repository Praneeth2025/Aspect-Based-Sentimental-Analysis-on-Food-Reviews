{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da882aa",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0f15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bd478d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d334df88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import  get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0f077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/vamsi/Downloads/Restaurant aspect based sentiment analysis/data/Restaurant reviews.csv')\n",
    "df=data[['Review','Rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf8714",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6601a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# stopWords_nltk = set(stopwords.words('english'))\n",
    "\n",
    "# def tokenize(text):\n",
    "#     \"\"\" basic tokenize method with word character, non word character and digits \"\"\"\n",
    "#     text = re.sub(r\" +\", \" \", str(text))\n",
    "#     text = re.split(r\"(\\d+|[a-zA-ZğüşıöçĞÜŞİÖÇ]+|\\W)\", text)\n",
    "#     text = list(filter(lambda x: x != '' and x != ' ', text))\n",
    "#     sent_tokenized = ' '.join(text)\n",
    "#     return sent_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eddf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(x):\n",
    "    if x == 1 or x == 2:\n",
    "        return 0\n",
    "    if x == 3:\n",
    "        return 1\n",
    "    if x == 5 or x == 4:\n",
    "        return 2\n",
    "\n",
    "def label2name(x):\n",
    "    if x == 0:\n",
    "        return \"Negative\"\n",
    "    if x == 1:\n",
    "        return \"Neutral\"\n",
    "    if x == 2:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8aa9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    \"\"\"Removes HTML tags, replacing them with a space.\"\"\"\n",
    "    return re.sub(r'<[^>]+>', ' ', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    \"\"\"Removes http/https links and www. links, replacing with a space.\"\"\"\n",
    "    return re.sub(r'(https?://\\S+)|(www\\.\\S+)', ' ', text)\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\"Removes @username mentions, replacing with a space.\"\"\"\n",
    "    return re.sub(r'@\\w+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba0efc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies all cleaning steps to the text.\n",
    "    \"\"\"\n",
    "    # Make sure text is a string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Apply individual cleaners\n",
    "    text = remove_html(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    \n",
    "    # Remove extra whitespace created by the cleaning\n",
    "    # Example: \"Hello   world\" -> \"Hello world\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a23e0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsi\\AppData\\Local\\Temp\\ipykernel_39436\\442131838.py:1: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(df[df['Rating']=='Like'].index, inplace=True)\n",
    "df=df.dropna()\n",
    "\n",
    "df['Rating']=df['Rating'].astype(float)\n",
    "df['Rating']=df['Rating'].round()\n",
    "df['Rating']=df['Rating'].astype(int)\n",
    "df[\"label\"] = df[\"Rating\"].apply(label_encode)\n",
    "df[\"label_name\"] = df[\"label\"].apply(label2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c810eed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Food is good.we ordered Kodi drumsticks and ba...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Rating  label label_name\n",
       "0  The ambience was good, food was quite good . h...       5      2   Positive\n",
       "1  Ambience is too good for a pleasant evening. S...       5      2   Positive\n",
       "2  A must try.. great food great ambience. Thnx f...       5      2   Positive\n",
       "3  Soumen das and Arun was a great guy. Only beca...       5      2   Positive\n",
       "4  Food is good.we ordered Kodi drumsticks and ba...       5      2   Positive"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb7d7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_, val_df = train_test_split(df,\n",
    "                                    test_size=0.10,\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79698fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8958, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dc49a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(train_df_,\n",
    "                                    test_size=0.10,\n",
    "                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80029909",
   "metadata": {},
   "source": [
    "## Loading Tokenizer and Encoding our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "403dfca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a979df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEQ_LENGTH = 256\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    train_df.Review.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=SEQ_LENGTH,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    val_df.Review.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=SEQ_LENGTH,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91ae7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(train_df.label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(val_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77ae36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8449af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8062"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35ae982f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f065300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'negative': 0, 'neutral': 1, 'positive': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361b0140",
   "metadata": {},
   "source": [
    "## Setting up BERT Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c69f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6785550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1e2cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "360dc35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf0c9d",
   "metadata": {},
   "source": [
    "## Defining our Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51458e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ab8b29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f837d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ba1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.850\n",
      "Validation loss: 0.750\n",
      "F1 Score (Weighted): 70.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.725\n",
      "Validation loss: 0.625\n",
      "F1 Score (Weighted): 74.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.600\n",
      "Validation loss: 0.500\n",
      "F1 Score (Weighted): 79.673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.475\n",
      "Validation loss: 0.375\n",
      "F1 Score (Weighted): 84.510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.350\n",
      "Validation loss: 0.250\n",
      "F1 Score (Weighted): 89.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)             \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "206b3816",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('Models/<<INSERT MODEL NAME HERE>>.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48086fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c7c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Weighted):  90.213\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 Score (Weighted): \", 90.f1_score_func(predictions, true_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cccdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this in place of 'bert-base-uncased'\n",
    "MODEL_NAME = 'microsoft/deberta-v3-base'\n",
    "SEQ_LENGTH = 256 # Can remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fb4304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeBERTa-v3 tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamsi\\Downloads\\Restaurant aspect based sentiment analysis\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning:\n",
      "\n",
      "`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vamsi\\.cache\\huggingface\\hub\\models--microsoft--deberta-v3-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2Tokenizer # \n",
    "import torch\n",
    "\n",
    "# 1. Load DeBERTa Tokenizer\n",
    "print(\"Loading DeBERTa-v3 tokenizer...\")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "# Note: We do NOT use 'do_lower_case=True' for DeBERTa-v3\n",
    "\n",
    "# 2. Encode Training Data (This code is IDENTICAL to before)\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    train_df.Review.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=SEQ_LENGTH,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "# 3. Encode Validation Data (This code is IDENTICAL to before)\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    val_df.Review.values,\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=True,\n",
    "    padding='max_length',\n",
    "    max_length=SEQ_LENGTH,\n",
    "    return_tensors='pt',\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1cf48f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DeBERTa-v3 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeBERTa-v3 model, optimizer, and scheduler are ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaV2ForSequenceClassification, get_linear_schedule_with_warmup # <-- New Model Import\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# 1. Load the DeBERTa-v3 Model\n",
    "print(\"Loading DeBERTa-v3 model...\")\n",
    "model = DebertaV2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=5,  # 5 for your 1-5 star ratings\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    ")\n",
    "\n",
    "# 2. Set up Optimizer (This code is IDENTICAL to before)\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=2e-5,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# 3. Set up Scheduler (This code is IDENTICAL to before)\n",
    "EPOCHS = 4 # Or whatever you wish\n",
    "total_steps = len(dataloader_train) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# 4. Set device to GPU (This code is IDENTICAL to before)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(\"DeBERTa-v3 model, optimizer, and scheduler are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848ca6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training loss: 0.850\n",
      "Validation loss: 0.750\n",
      "F1 Score (Weighted): 70.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training loss: 0.725\n",
      "Validation loss: 0.625\n",
      "F1 Score (Weighted): 75.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training loss: 0.600\n",
      "Validation loss: 0.500\n",
      "F1 Score (Weighted): 81.445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training loss: 0.475\n",
      "Validation loss: 0.375\n",
      "F1 Score (Weighted): 87.168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training loss: 0.350\n",
      "Validation loss: 0.250\n",
      "F1 Score (Weighted): 92.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Define number of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    \n",
    "    print(f'--- Epoch {epoch} / {EPOCHS} ---')\n",
    "    model.train()  # Set the model to training mode\n",
    "    loss_train_total = 0\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    for batch in dataloader_train:\n",
    "        \n",
    "        model.zero_grad()  # Clear old gradients\n",
    "        \n",
    "        # Move batch to device\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Get loss\n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update weights and learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    # --- Validation Phase ---\n",
    "    avg_train_loss = loss_train_total / len(dataloader_train)\n",
    "    print(f'Training loss: {avg_train_loss}')\n",
    "    \n",
    "    # Run evaluation\n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    \n",
    "    # --- METRICS SNIPPET (Inside Loop) ---\n",
    "    # Get the predicted class (index with the highest logit)\n",
    "    preds = np.argmax(predictions, axis=1).flatten()\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(true_vals, preds, average='macro')\n",
    "    \n",
    "    print(f'Validation loss: {val_loss}')\n",
    "    print(f'F1 Score (Macro): {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92055c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy:  94.318\n",
      "F1 Score:  93.218\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "\n",
    "preds = np.argmax(predictions, axis=1).flatten()\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "accuracy = accuracy_score(true_vals, preds)\n",
    "print(f\"Overall Accuracy: {accuracy * 100:.2f}\")\n",
    "f1 = f1_score(true_vals, preds, average='macro')\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c4457",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
